# Benchmark of core capabilities

[View the Taxonomy](taxonomy_v4.pdf)

## [Knowledge-Benchmarks](https://github.com/ALEX-nlp/Knowledge-Benchmarks)
This page organizes LLM knowledge-evaluation benchmarks into four major types—**Breadth**, **Depth**, **Truthfulness**, and **Dynamic/Timely**—and lists representative datasets for each. We’ve also added a handful of the latest 2024–2025 works.

## [Reasoning-Benchmarks](https://github.com/ALEX-nlp/Reasoning-Benchmark/tree/main)
Reasoning benchmarks probe LLMs’ structured thought across multiple domains—mathematics, coding, commonsense, long-context comprehension, formal logic, hierarchical planning, and miscellaneous symbolic tasks.

## [Instruction-Following-Benchmarks](https://github.com/ALEX-nlp/Instruction-Following-Benchmarks)
Instruction-following benchmarks have evolved from single-task NLP sets to rich, real-world, and automated evaluations. Early datasets focused on mapping input to output on held-out tasks, giving way to instruction-tuning collections and prompt generalization. Modern evaluations incorporate human prompts, automated judges, style-control, and constraint-based tests. We also highlight recent benchmarks targeting specialized domains, evaluator robustness, and long-context stability.

## [Safety-Benchmarks](https://github.com/ALEX-nlp/Safety-Benchmarks)
Safety evaluation benchmarks ensure LLMs avoid harmful, unethical, or biased outputs by testing across four directions: Content Safety, Multi‐Dimensional Trustworthiness, Adversarial Robustness, and Agentic Safety.

---


